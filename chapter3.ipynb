{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. \n",
    "Try to bulid a classifier for the MNIST dataset that achieves over 97% accuracy on the test set. Hint: the KNeighborsClassifier works quite well for this task; you just need to find good hyperparameter values (try a grid search on the weights and n_neighbors hyperparameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as skl\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml('mnist_784', as_frame=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784) (70000,)\n"
     ]
    }
   ],
   "source": [
    "X, y = mnist.data, mnist.target\n",
    "print(X.shape, y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train/test\n",
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9819166666666667\n"
     ]
    }
   ],
   "source": [
    "#Start by training a simple KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf.fit(X_train, y_train)\n",
    "\n",
    "#Print the train accuracy\n",
    "y_train_pred = knn_clf.predict(X_train)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_train, y_train_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9676 , 0.9671 , 0.96755])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate out-of-sample accuracy with cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(knn_clf, X_train, y_train, cv = 3, scoring = \"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, as we can see, we have a out-of-sample prediction accuracy slightly less than  97%. Lets use gridsearch and see if we can increase this value, as suggested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 4, 'weights': 'distance'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [{'weights': ['uniform', 'distance'], 'n_neighbors': [4, 6, 8]}]\n",
    "\n",
    "grid_search = GridSearchCV(knn_clf, param_grid, cv = 3, scoring = 'accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9703500000000002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.9709 , 0.9698 , 0.97035])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets check our best score result:\n",
    "print(grid_search.best_score_)\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "#check accuracy on cross validation\n",
    "cross_val_score(best_estimator, X_train, y_train, cv = 3, scoring = \"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before calculating accuracy on test set, lets try a random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96525, 0.9637 , 0.96615])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "#Calculate out-of-sample accuracy with cross-validation\n",
    "cross_val_score(rf_clf, X_train, y_train, cv = 3, scoring = \"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xande\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\ma\\core.py:2881: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 50, 'n_estimators': 100}\n",
      "0.9648333333333333\n"
     ]
    }
   ],
   "source": [
    "#Since we got low accuracy, lets try to tune the hyperparameters with grid search\n",
    "param_grid = [{'n_estimators': [10, 50, 100], 'max_features': [5, 10, 50, 100]}]\n",
    "grid_search_rf = GridSearchCV(rf_clf, param_grid, cv =3, scoring = \"accuracy\")\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "print(grid_search_rf.best_params_)\n",
    "print(grid_search_rf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9714\n"
     ]
    }
   ],
   "source": [
    "#So out best model remains the KNN with 97.03% accuracy\n",
    "#Lets check the accuracy on the test set\n",
    "y_test_pred = best_estimator.predict(X_test)\n",
    "print(accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) \n",
    "Write a function that can shift MNIST image in any directions (left, right, up or down) by one pixel. Then, for each image in the training set, create four shifted copies (one per direction) and add them to the training set. Finally, train your best model on this expanded training set and measure its accuracy on the test set. You should observe that your model performs even better now! This technique of artificially growing the training set is called *data augmentation* or *training set expansion*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.ndimage import shift\n",
    "\n",
    "X_train_right = [shift(image.reshape(28,28), (0,1)).reshape(784) for image in X_train]\n",
    "X_train_left = [shift(image.reshape(28,28), (0,-1)).reshape(784) for image in X_train]\n",
    "X_train_up = [shift(image.reshape(28,28), (1,0)).reshape(784) for image in X_train]\n",
    "X_train_down = [shift(image.reshape(28,28), (-1,0)).reshape(784) for image in X_train]\n",
    "\n",
    "X_test_right = [shift(image.reshape(28,28), (0,1)).reshape(784) for image in X_test]\n",
    "X_test_left = [shift(image.reshape(28,28), (0,-1)).reshape(784) for image in X_test]\n",
    "X_test_up = [shift(image.reshape(28,28), (1,0)).reshape(784) for image in X_test]\n",
    "X_test_down = [shift(image.reshape(28,28), (-1,0)).reshape(784) for image in X_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Use the authors plot function to see shifted image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_digit(image):\n",
    "    image = image.reshape(28,28)\n",
    "    plt.imshow(image, cmap = \"binary\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "fig, axs = plt.subplots(nrows = 2, ncols = 2)\n",
    "plt.subplot(2,2,1); plot_digit(X_train_right[0])\n",
    "plt.subplot(2,2,2); plot_digit(X_train_left[0])\n",
    "plt.subplot(2,2,3); plot_digit(X_train_up[0])\n",
    "plt.subplot(2,2,4); plot_digit(X_train_down[0])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it is working just fine, lets estimate the model with the augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9761\n"
     ]
    }
   ],
   "source": [
    "X_train_aug = X_train + X_train_right + X_train_left + X_train_up + X_train_down\n",
    "X_test_aug = X_test + X_test_right + X_test_left + X_test_up + X_test_down\n",
    "\n",
    "best_estimator.fit(X_train_aug, y_train)\n",
    "y_test_pred_aug = best_estimator.predict(X_test_aug)\n",
    "print(accuracy_score(y_test, y_test_pred_aug))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.\n",
    "Tackle the Titanic dataset. A great place to start in on Kaggle. Alternatively, you can download the data from [Here](\"https://homl.info/titanic.tgz\") and unzip this tarball like you did for the housing data in Chapter 2. This will give you two CSV files, train.csv and test.csv, which you can load using panda.read.csv. The goal is to train a classifier that can predict the Survived column based on the other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
      "Your Kaggle username:Your Kaggle Key:Downloading titanic.zip to .\\titanic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34.1k/34.1k [00:00<00:00, 479kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting archive .\\titanic/titanic.zip to .\\titanic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Download Titanic data using an API\n",
    "import opendatasets as od\n",
    "import pandas as plot_digit\n",
    "od.download(\"https://www.kaggle.com/c/titanic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Pclass                                               Name  \\\n",
      "0            1       3                            Braund, Mr. Owen Harris   \n",
      "1            2       1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
      "2            3       3                             Heikkinen, Miss. Laina   \n",
      "3            4       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
      "4            5       3                           Allen, Mr. William Henry   \n",
      "\n",
      "      Sex   Age  SibSp  Parch            Ticket     Fare Cabin Embarked  \\\n",
      "0    male  22.0      1      0         A/5 21171   7.2500   NaN        S   \n",
      "1  female  38.0      1      0          PC 17599  71.2833   C85        C   \n",
      "2  female  26.0      0      0  STON/O2. 3101282   7.9250   NaN        S   \n",
      "3  female  35.0      1      0            113803  53.1000  C123        S   \n",
      "4    male  35.0      0      0            373450   8.0500   NaN        S   \n",
      "\n",
      "   HasCabin  \n",
      "0     False  \n",
      "1      True  \n",
      "2     False  \n",
      "3      True  \n",
      "4     False  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load the train and test datasets\n",
    "train = pd.read_csv('titanic/train.csv')\n",
    "test = pd.read_csv('titanic/test.csv')\n",
    "\n",
    "X_train = train.drop(\"Survived\", axis = 1)\n",
    "y_train = train[\"Survived\"]\n",
    "X_test = test\n",
    "\n",
    "#Create a column indicating if the passenger has a cabin\n",
    "X_train['HasCabin'] = X_train['Cabin'].isnull() == False\n",
    "X_test['HasCabin'] = X_test['Cabin'].isnull() == False\n",
    "\n",
    "print(X_train.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 12) (418, 12)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Pclass       891 non-null    int64  \n",
      " 2   Name         891 non-null    object \n",
      " 3   Sex          891 non-null    object \n",
      " 4   Age          714 non-null    float64\n",
      " 5   SibSp        891 non-null    int64  \n",
      " 6   Parch        891 non-null    int64  \n",
      " 7   Ticket       891 non-null    object \n",
      " 8   Fare         891 non-null    float64\n",
      " 9   Cabin        204 non-null    object \n",
      " 10  Embarked     889 non-null    object \n",
      " 11  HasCabin     891 non-null    bool   \n",
      "dtypes: bool(1), float64(2), int64(4), object(5)\n",
      "memory usage: 77.6+ KB\n"
     ]
    }
   ],
   "source": [
    "#See if we have missing values\n",
    "print(X_train.shape, X_test.shape)\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have some missing data, we should inpute something to not miss these rows. Lets use the nearest neighbors values. Note that it will only work for the Age column, since Cabin and Embarked are not numeric variables, so we must create a pipeline for numerical and another for categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.73010796,  0.82737724, -0.562414  , ...,  1.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-1.72622007, -1.56610693,  0.60893948, ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       [-1.72233219,  0.82737724, -0.26957563, ...,  1.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 1.72233219,  0.82737724,  0.20628672, ...,  1.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 1.72622007, -1.56610693, -0.26957563, ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       [ 1.73010796,  0.82737724,  0.16968192, ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "\n",
    "#Create pipelines\n",
    "from sklearn.pipeline import Pipeline\n",
    "#pipeline for numerical attributes\n",
    "num_pipeline = Pipeline([\n",
    "    ('impute', KNNImputer(n_neighbors=2)),\n",
    "    ('standardize', StandardScaler())\n",
    "])\n",
    "\n",
    "#pipeline for categorical attributes\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy = \"constant\", fill_value = \"missing\")),\n",
    "    (\"one-hot\", OneHotEncoder(sparse_output=False))\n",
    "])\n",
    "\n",
    "#also define some columns to drop and categorical variables to keep\n",
    "columns_to_drop = [\"Name\", \"Ticket\", \"Cabin\"]\n",
    "cat_var_keep = [\"Sex\", \"Embarked\"]\n",
    "\n",
    "#create aggregate pipeline\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('column_dropper', 'drop', columns_to_drop),\n",
    "    ('num', num_pipeline, selector(dtype_include = np.number)),\n",
    "    ('cat', cat_pipeline, cat_var_keep)\n",
    "], remainder = 'passthrough')\n",
    "\n",
    "#Now we are ready to preprocess our data\n",
    "X_train_prep = preprocessor.fit_transform(X_train)\n",
    "X_train_prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets train some classification models and choose the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'random_forest__max_features': 2}\n",
      "0.818214731585518\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#RF pipeline with grid search\n",
    "rf_pipeline = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"random_forest\", RandomForestClassifier(random_state = 50))\n",
    "])\n",
    "param_grid = [{'random_forest__max_features':[2,4,6]}]\n",
    "grid_search_rf = GridSearchCV(rf_pipeline, param_grid, cv =10, scoring = \"accuracy\")\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "print(grid_search_rf.best_params_)\n",
    "print(grid_search_rf.best_score_)\n",
    "best_rf = grid_search_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ok classification with Random Forest. Lets try a logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.7923470661672909)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#lf pipeline\n",
    "rf_pipeline = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"logistic_reg\", LogisticRegression())\n",
    "])\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(rf_pipeline, X_train, y_train, cv = 10, scoring = \"accuracy\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An even worse classification with a logistic regression. Lets try a SGD, with both lasso and ridge regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sgd_class__loss': 'log_loss', 'sgd_class__penalty': 'l1'}\n",
      "0.7900749063670411\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd_pipeline = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"sgd_class\", SGDClassifier(random_state = 50))\n",
    "])\n",
    "param_grid = [{'sgd_class__penalty': ['l2','l1'],\n",
    "               'sgd_class__loss': ['hinge', 'log_loss', 'perceptron']}]\n",
    "grid_search_sgd = GridSearchCV(sgd_pipeline, param_grid, cv = 10, scoring = \"accuracy\")\n",
    "grid_search_sgd.fit(X_train, y_train)\n",
    "print(grid_search_sgd.best_params_)\n",
    "print(grid_search_sgd.best_score_)\n",
    "best_sgd = grid_search_sgd.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even Worse. Finally, lets try a Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'svc_class__gamma': 'auto', 'svc_class__kernel': 'rbf'}\n",
      "0.8249313358302122\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc_pipeline = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    ('svc_class', SVC(random_state = 50))\n",
    "\n",
    "])\n",
    "param_grid = [{\n",
    "    'svc_class__kernel' : ['rbf', 'linear', 'poly'],\n",
    "    'svc_class__gamma' : ['scale', 'auto']\n",
    "}]\n",
    "grid_search_svc = GridSearchCV(svc_pipeline, param_grid, cv = 10, scoring = \"accuracy\")\n",
    "grid_search_svc.fit(X_train, y_train)\n",
    "print(grid_search_svc.best_params_)\n",
    "print(grid_search_svc.best_score_)\n",
    "best_svc = grid_search_svc.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build a Spam classifier\n",
    "\n",
    "a) Download examples of spam and ham from [Here](https://spamassassin.apache.org/old/publiccorpus/)\n",
    "\n",
    "## I REMAIN TO FINISH THIS QUESTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import tarfile\n",
    "\n",
    "url = 'https://spamassassin.apache.org/old/publiccorpus/20021010_easy_ham.tar.bz2'\n",
    "target_path = '../datasets/20021010_easy_ham.tar.bz2'\n",
    "\n",
    "#get data\n",
    "response = requests.get(url, stream = True)\n",
    "#save data local\n",
    "with(open(target_path, 'wb')) as file:\n",
    "    file.write(response.raw.read())\n",
    "\n",
    "#unzip and extract\n",
    "with tarfile.open(target_path, 'r:bz2') as tar:\n",
    "    tar.extractall()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
