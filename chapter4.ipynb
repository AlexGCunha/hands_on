{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Which linear regresssion training algorithm can you use if you have a training set with millions of features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case (and supposing you dont have millions more observations), you should opt for Gradient Descent Methods. Closed form solutions are infeasible because they require you to have at least as many observations as features (m > n) in order to $X^{T}X$ to be invertible. SVD also should be avoided since it would take too long to (because of the many features) calculate the pseudinverse. Hence, you shoul use gradient descent methods. More speciffically, you should use either SGD or Mini Batch Gradient Descent for efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Supppose the features in your training set have very different scales. Which algorithms might suffer from this, and how? What can you do about it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any gradient descent training algorithim would suffer, taking much longer to converge. That is because these algorithms use the same learning rate for every feature, hence, features with smaller scales take proportionally bigger steps at each iteration. To solve you should just scale your features, like using StandardScaler.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Can gradient descent get stuck in a local minimum when training a logistic regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, since the cost function of a logistic regression is stricly convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Do all gradient descent algorithms lead to the same model, provided you let them run long enough?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you let then run long enough, and the learning rate is not high enough so they diverge, they will all converge to the minimum or, in the case of SGD and Mini-Batch GD, cycle through the minimum. SGD and Mini_batch will converge if you decrease the learning rate at each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Suppose you use batch gradient descent and you plot the validation error at every epoch. If you notice that the validation error consistently goes up, what is likely going on? How can you fix this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It depends; if the training error is going down, then the model is probably overfitting to the data. There are a couple of things you can do. First, you could use some kind of regularization to penalyze some weights and consequently overfitting, or yo could try to get more observations. If you notice that the validation error started by decreasing in the beggining and then started increasing, you could also use an early stop procedure. On the other hand, if the training error is also going up, then the learning rate is probably too high and you should first decrease that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Is it a good idea to stop mini-batch gradient descent immediately when the balidation error goes up?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No. Since you are not using the full features dataset, it could be that in this specific iteration, the error has gone up because the observations are a bit more different than the ones already trained in the model, and so the error goes up and you should not stop. But if after some iterations the error continues to increase, then you should stop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Which gradient descent algorithim (among those we discussed) will reach the vicinity of the optimal solution the fastest? Wchich will actually converge? How can you make the others converge as well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SGD. Since, in each epoch, SGD works with just one row of observations (instance), it is able to run each epoch much faster than Batch GD for example, but not necessarily it will converge. Batch GD will converge as long as the learning rate is not too big. One can make SGD and Mini-Batch GD converge by decreasing the learning rate over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Suppose you are using polynomial regression. You plot the learning curves and you notice that there is a large gap between the training error and the validation error. What is happening? What are three ways to solve this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are probably overfitting. You could use some regularization technique, like the reidge regression to penalyze weights and decrease overfitting. Additionaly, you could try to get more instances so your model has more training observations. Finally, you could use some grid search (with cross validation) to find a better suiting polynomial degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9) Suppose you are using ridge regression and you notice that the training error and the validation error are almost equal and fairly high. Would you say that the model suffers from high bias or high variance? Should you increase the regularization hyperparameter $\\alpha$ or reduce it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model if probably suffering of high bias. Since the training and validation error are similar, the model is not overfitting. But because both errors are high, the estimation is poor. Hence, you should decrease $\\alpha$ and allow a bit more of variance in order to decrease your bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10) Why would you want to use:\n",
    "    a. Ridge regression instead of plain linear regression (i.e., without any regularization)?\n",
    "    b. Lasso instead of ridge regression?\n",
    "    c. Elastic new instead of lasso regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Even though linear regression is not too prone to overfitting, it could happen if you have a very wide dataset (too many features for too few instances). In this case, or any other with overfitting problems, you could try to decrease this overfit with a regularization technique, such as Ridge regression.\n",
    "b. Lasso can be used for feature selection. That is, since lasso sets some weights to zero, not all features will have an impact on the estimation. So if you think not all featreus are important to predict, you should use lasso.\n",
    "c. Elastic new is a middle ground between lasso and ridge, depending o the r ratio (l1_ratio), it will give results more similar to laso or to ridge. Hence, if you want a model that has some of the benefits of both, without so much of their problems (like lasso`s bad behaviour with few instances), you should opt for elastic net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11) Suppose you want to classify pictures as outdoor/indoor and daytime/nighttime. Should you implement two logistic regression classifiers or one softmax regression classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case you should run two logistic regression, of for outdoor/indoor and one for daytime/nighttime. That is the case because softmax is a multiclass classifier, and not multioutput. Since we have two outputs (location and time of day), you cant use softmax to classify both outputs at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12) Implement batch gradient descent with early stopping for softmax regression without using Scikit-Learn, only Numpy. Use it on a classification taske such as the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class softmax_classification:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.m = X.shape[0] #observations (instances)\n",
    "        self.n = X.shape[1] #parameters\n",
    "        self.K = len(np.unique(self.y)) #distinct label classes\n",
    "        self.Theta = np.random.randn(K,n) #1 parameter row for each possible label\n",
    "\n",
    "\n",
    "    #Calculate score for each possible label\n",
    "    def softmax_score(self):\n",
    "        #matrix to save scores for each instance\n",
    "        scores = np.zeros(shape = (self.K,self.m))\n",
    "        for i in range(self.K):\n",
    "            scores[i,:] = np.matmul(self.X, self.Theta[i].T)\n",
    "\n",
    "        return scores\n",
    "\n",
    "    #softmax function to calculate probabilities\n",
    "    def softmax_function(scores):\n",
    "        #exponentiate scores\n",
    "        scores_exp = np.exp(scores)\n",
    "        #divide by row sum\n",
    "        probs = scores_exp/scores_exp.sum(axis = 0)\n",
    "        return probs\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.11920292202211755)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.11920292, 0.11920292],\n",
       "        [0.88079708, 0.88079708]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teste = np.matrix([[1,2], [3,4]])\n",
    "def softmax_function(scores):\n",
    "        #exponentiate scores\n",
    "        scores_exp = np.exp(scores)\n",
    "        #divide by row sum\n",
    "        probs = scores_exp/scores_exp.sum(axis = 0)\n",
    "        return probs\n",
    "softmax_function(teste)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
