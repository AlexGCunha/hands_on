{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) This question is about using a webtool, without the need for any written answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Draw an ANN using the original artificial neurons (like the ones in Figure 10-3) that computes A ⊕ B = (A ∧ ¬B) ∨ (¬ A V B)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done on paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Why is it generally prefrable to use a logistic regression classifier rather than a classic perceptron (i.e., a single layer of threshold logic units trained using the perceptron training algorithm)? How can you tweak a perceptron to make it equivalent to a logistic regression classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because a common perceptron supposes linearly separable datasets. That is, if the decision boundary is not linear, than a classic perceptron won't be able to correctlt classify instances, whereas a logistic regression could potentially do. If you change the output activation function to a sigmoid, and train it with gradient descent or other equivalent method, than it will converge to the same result as in a usual logistic regression, with class probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Why was the sigmoid activation function a key ingredient in training the first MLPs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the sigmoid's derivative is always non-zero, gradient descent methods will keep iterating and converging to global minimum, whereas other activation functions, like step functions, where the derivative is always equal to zero, it cannot converge to global minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Name three popular activation functions. Can you draw them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relu is 0 for values lower than 0 45 degrees linear for values higher than 0 (y = 0 if x <= 0; y = x if x >0).\n",
    "\n",
    "Sigmoid is a S-shaped curve that takes values from -infinity to +infinity and maps then to (0,1)\n",
    "\n",
    "Linear activation functions make a linear transformation on the inputs, mapping then to a single output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Suppose you have an MLP composed of one input layer with 10 passthough neurons, followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial neurons. All artificial neurons use the ReLU activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) What is the shape of the input matrix $\\textbf{X}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you have m instances. Then $\\textbf{X}$ is mx10. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) What are the shapes of the hidden layer's weight matrix $\\textbf{W}_h$ and bians vector $\\textbf{b}_h$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each hidden layer is connected to 10 neurons, where each column contains the 10 weights of a single neuron, so $\\textbf{W}_h$ is 10x50.\n",
    "\n",
    "Since we have 1 bias for each neuron, our vector $\\textbf{b}_h$ has size 50. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) What are the shapes of the output layer's weight matrix $\\textbf{W}_0$ and bias vector $\\textbf{b}_0$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each output layer is connected to 50 neuros, so $\\textbf{W}_0$ has size 50x3. Since we have 3 neuros on the output layer, $\\textbf{b}_0$ has size 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) What is the shape of the network's output matrix $\\textbf{Y}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, suppposing we have m instances, then $\\textbf{Y}$ is mx3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Write the equation that computed the newtork's output matrix $\\textbf{Y}$ as a function of $\\textbf{X}$, $\\textbf{W}_h$, $\\textbf{b}_h$, $\\textbf{W}_0$ and $\\textbf{b}_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Y = ReLU(ReLU(\\textbf{XW}_h + \\textbf{b}_h)\\textbf{W}_0+ \\textbf{b}_0)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) How many neurons do you need in the output layer if you want to classify email into spam or ham? What activation funciton shoud you use in the output layer? If instead you want to tackle MNIST, how many neuros do you need i the output layer, and which activation function should you use? What about for getting your network to predict housing prices, as in Chapter 2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify an email into either spam or ham, you can use a single-neuron output layer with a sigmoid activation function. In the MNIST case, where your classification isn't binary, you would need 10 neurons, one for each possible output, with the softmax activation function. For housing prices prediction, where the output is continuous, you should use one neuron with a linear activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) What is backpropagation and how does it work? What is the difference between backpropagation and revese-mode autodiff?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation is one way to train MLPs (and possibly other deep learning algorithms), that is, to find the weights that best fit the data to sample. It starts by computing the estimates with the weigths from previous iteration (or random weights in the first one). Then, for the output layer, it calculates the derivative of the loss function with respect to each previous neuron, and from each previous neuron to the ones immediately prior and so one until the first layer. Then, with the chain rule, it is able to calculate the derivative of the loss function with respect to each weight from prior neurons (i.e., calculate the gradient of the loss function), and update these weights in a gradient descent step. It repeats until convergence. Reverse-mode autodiff is a technique used in backpropagation to calculate the gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9) Can you list all the hyperparameters you can tweak in a basic MLP? If the MLP overfits the training data, how could you tweak these hyperparameter to try to solve the problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters and tweaks to decrease overfiting:\n",
    "\n",
    "- Number of hidden layers: decrease \n",
    "\n",
    "- Number of neurons per hidden layer: decrease\n",
    "\n",
    "- Learning rate: \n",
    "\n",
    "- Optimizer method: \n",
    "\n",
    "- Activation Funtion:\n",
    "\n",
    "- Batch Size: \n",
    "\n",
    "- Number of iterations: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10) Train a deep MLP on the MNIST dataset (you can load it using tf.keras.datasets.mnist.load_data()). See if you can get over 98% accuracy by manually tuning the hyperparameters. Try searching for the optimal learning rate by using the approach presented in this chapter (i.e., by growing the learning rate exponentially, plotting the loss, and finding the point where the loss shoots up). Next, try tuning the hyperparameters using Keras Tuner with all the bells and whistles - save checkpoints, use early stopping, and plot learning curves using TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
